#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¿ç˜¤ vs æ­£å¸¸æ ·æœ¬ PTM æ•ˆåº”æ¯”è¾ƒåˆ†æ

åˆ†æç‰¹å®šPTMåœ¨è‚¿ç˜¤æ ·æœ¬vsæ­£å¸¸æ ·æœ¬ä¸­å¯¹ç†åŒ–æ€§è´¨çš„ç›¸å¯¹å½±å“
æ–¹æ³•ï¼šratio = (è¯¥PTMè‚½ä¸­ä½æ•°) Ã· (åŒä¸€æ ·æœ¬ä¸­æœªä¿®é¥°è‚½ä¸­ä½æ•°)ï¼Œå†æ¯”è¾ƒ Tumor vs Normal
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥statsmodelsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€å•çš„å¤šé‡æ¯”è¾ƒæ ¡æ­£
try:
    from statsmodels.stats.multitest import multipletests
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
    print("âš ï¸  statsmodelsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„Bonferroniæ ¡æ­£")

# å¯¼å…¥ç°æœ‰çš„åˆ†ææ¨¡å—
try:
    from peptide_properties_analyzer import PeptidePropertiesAnalyzer
    HAS_PROPERTIES_ANALYZER = True
except ImportError:
    HAS_PROPERTIES_ANALYZER = False
    print("âš ï¸  peptide_properties_analyzeræœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„ç†åŒ–æ€§è´¨è®¡ç®—")

class TumorNormalPTMAnalyzer:
    """è‚¿ç˜¤vsæ­£å¸¸æ ·æœ¬PTMæ•ˆåº”æ¯”è¾ƒåˆ†æå™¨"""
    
    def __init__(self):
        if HAS_PROPERTIES_ANALYZER:
            self.properties_analyzer = PeptidePropertiesAnalyzer()
        else:
            self.properties_analyzer = None
        
        # ç›®æ ‡ä¿®é¥°ç±»å‹
        self.target_modifications = [
            'Phospho[S]', 'Phospho[T]', 'Phospho[Y]',
            'Acetyl[K]', 'Methyl[K]', 'Dimethyl[K]', 'Trimethyl[K]',
            'Deamidated[N]', 'Deamidated[Q]',
            'Ubiquitination[K]', 'Citrullination[R]'
        ]
        
        # ç†åŒ–æ€§è´¨åˆ—å
        self.property_columns = [
            'corrected_pi', 'corrected_charge_at_ph7', 
            'corrected_hydrophobicity_kd', 'corrected_molecular_weight'
        ]
        
        # ç†åŒ–æ€§è´¨æ˜¾ç¤ºåç§°
        self.property_names = ['pI', 'Net_Charge', 'Hydrophobicity_KD', 'Molecular_Weight']
        
        # è¾“å‡ºç›®å½•
        self.output_dir = 'tumor_vs_normal_ptm_results'
        os.makedirs(self.output_dir, exist_ok=True)
    
    def load_and_prepare_data(self, dataset_id: str):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_id}")
        
        # åŠ è½½summaryæ–‡ä»¶è·å–æ ·æœ¬ç±»å‹ä¿¡æ¯
        summary_file = f'tumor_summary/{dataset_id}_summary.csv'
        if not os.path.exists(summary_file):
            print(f"âŒ æ‰¾ä¸åˆ°summaryæ–‡ä»¶: {summary_file}")
            return None
        
        summary_df = pd.read_csv(summary_file)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Tumorå’ŒNormalæ ·æœ¬
        sample_types = summary_df['Type'].unique()
        if not ('Tumor' in sample_types and 'Normal' in sample_types):
            print(f"âš ï¸  æ•°æ®é›† {dataset_id} æ²¡æœ‰åŒæ—¶åŒ…å«Tumorå’ŒNormalæ ·æœ¬")
            print(f"   æ ·æœ¬ç±»å‹: {sample_types}")
            return None
        
        print(f"âœ… å‘ç°æ ·æœ¬ç±»å‹: {sample_types}")
        print(f"   Tumoræ ·æœ¬: {len(summary_df[summary_df['Type'] == 'Tumor'])} ä¸ª")
        print(f"   Normalæ ·æœ¬: {len(summary_df[summary_df['Type'] == 'Normal'])} ä¸ª")
        
        # åŠ è½½åŸå§‹æ•°æ®
        dataset_dir = f'{dataset_id}_human'
        if not os.path.exists(dataset_dir):
            print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®ç›®å½•: {dataset_dir}")
            return None
        
        # å¤„ç†æ‰€æœ‰æ ·æœ¬æ–‡ä»¶
        all_data = []
        
        for _, row in summary_df.iterrows():
            file_name = row['File Name']
            sample_name = row['Sample Name']
            sample_type = row['Type']
            
            file_path = os.path.join(dataset_dir, file_name)
            if not os.path.exists(file_path):
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            try:
                # è¯»å–å•ä¸ªæ–‡ä»¶
                sample_data = pd.read_csv(file_path, sep='\t')

                # è°ƒè¯•ï¼šæ£€æŸ¥åˆ—å
                if len(all_data) == 0:  # åªå¯¹ç¬¬ä¸€ä¸ªæ–‡ä»¶æ‰“å°
                    print(f"  ç¬¬ä¸€ä¸ªæ–‡ä»¶åˆ—å: {sample_data.columns.tolist()}")
                    print(f"  ç¬¬ä¸€ä¸ªæ–‡ä»¶å½¢çŠ¶: {sample_data.shape}")
                    if 'Modification' in sample_data.columns:
                        mod_sample = sample_data['Modification'].dropna().head(3)
                        print(f"  ä¿®é¥°åˆ—æ ·ä¾‹: {mod_sample.tolist()}")

                # æ·»åŠ æ ·æœ¬ä¿¡æ¯
                sample_data['sample_id'] = sample_name
                sample_data['sample_type'] = sample_type
                sample_data['dataset_id'] = dataset_id

                all_data.append(sample_data)

            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                continue
        
        if not all_data:
            print(f"âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®æ–‡ä»¶")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_data = pd.concat(all_data, ignore_index=True)
        print(f"ğŸ“Š åˆå¹¶æ•°æ®: {len(combined_data)} æ¡è®°å½•")
        
        # æ•°æ®é¢„å¤„ç†
        processed_data = self.preprocess_data(combined_data)
        
        return processed_data
    
    def preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
        
        # é‡å‘½ååˆ—ä»¥åŒ¹é…æœŸæœ›æ ¼å¼
        column_mapping = {
            'Sequence': 'sequence',
            'Modification': 'modification_info'
        }

        print(f"  åˆå¹¶ååˆ—å: {data.columns.tolist()}")
        print(f"  æ£€æŸ¥Modificationåˆ—æ˜¯å¦å­˜åœ¨: {'Modification' in data.columns}")

        for old_col, new_col in column_mapping.items():
            if old_col in data.columns:
                print(f"  é‡å‘½å {old_col} -> {new_col}")
                data = data.rename(columns={old_col: new_col})
            else:
                print(f"  âš ï¸  åˆ— {old_col} ä¸å­˜åœ¨")

        print(f"  é‡å‘½åååˆ—å: {data.columns.tolist()}")
        if 'modification_info' in data.columns:
            mod_sample = data['modification_info'].dropna().head(3)
            print(f"  é‡å‘½ååä¿®é¥°åˆ—æ ·ä¾‹: {mod_sample.tolist()}")
        
        # è§£æä¿®é¥°ä¿¡æ¯
        data = self.parse_modifications(data)
        
        # è®¡ç®—ç†åŒ–æ€§è´¨
        data = self.calculate_properties(data)
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        data = data.dropna(subset=['sequence'] + self.property_columns)
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(data)} æ¡æœ‰æ•ˆè®°å½•")
        
        return data
    
    def parse_modifications(self, data: pd.DataFrame) -> pd.DataFrame:
        """è§£æä¿®é¥°ä¿¡æ¯"""
        print("ğŸ” è§£æä¿®é¥°ä¿¡æ¯...")

        # åˆå§‹åŒ–ä¿®é¥°ç›¸å…³åˆ—
        data['modification_type'] = ''
        data['num_modifications'] = 0

        # è§£æModificationåˆ—
        for idx, row in data.iterrows():
            mod_info = str(row.get('modification_info', ''))

            # è°ƒè¯•å‰å‡ è¡Œ
            if idx < 5:
                print(f"  è¡Œ {idx}: ä¿®é¥°ä¿¡æ¯ = '{mod_info}'")

            if pd.isna(mod_info) or mod_info == '' or mod_info == 'nan':
                # æœªä¿®é¥°
                data.at[idx, 'modification_type'] = ''
                data.at[idx, 'num_modifications'] = 0
            else:
                # è§£æä¿®é¥°
                modifications = []
                mod_count = 0

                # åˆ†å‰²å¤šä¸ªä¿®é¥°ï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰
                mod_parts = mod_info.split(';')

                for part in mod_parts:
                    part = part.strip()
                    if not part:
                        continue

                    # æ ¼å¼: position,ModificationType
                    if ',' in part:
                        try:
                            pos, mod_type = part.split(',', 1)
                            mod_type = mod_type.strip()

                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡ä¿®é¥°
                            target_found = None

                            # ç£·é…¸åŒ–ä¿®é¥°
                            if 'Phospho' in mod_type:
                                if '[S]' in mod_type:
                                    target_found = 'Phospho[S]'
                                elif '[T]' in mod_type:
                                    target_found = 'Phospho[T]'
                                elif '[Y]' in mod_type:
                                    target_found = 'Phospho[Y]'

                            # ä¹™é…°åŒ–ä¿®é¥°
                            elif 'Acetyl' in mod_type:
                                if '[K]' in mod_type:
                                    target_found = 'Acetyl[K]'
                                elif 'ProteinN-term' in mod_type:
                                    target_found = 'Acetyl[K]'  # Nç«¯ä¹™é…°åŒ–ä¹Ÿç®—

                            # ç”²åŸºåŒ–ä¿®é¥°
                            elif 'Methyl' in mod_type:
                                if 'Dimethyl' in mod_type and '[K]' in mod_type:
                                    target_found = 'Dimethyl[K]'
                                elif 'Trimethyl' in mod_type and '[K]' in mod_type:
                                    target_found = 'Trimethyl[K]'
                                elif '[K]' in mod_type:
                                    target_found = 'Methyl[K]'

                            # è„±é…°èƒºä¿®é¥°
                            elif 'Deamidated' in mod_type:
                                if '[N]' in mod_type:
                                    target_found = 'Deamidated[N]'
                                elif '[Q]' in mod_type:
                                    target_found = 'Deamidated[Q]'

                            if target_found:
                                modifications.append(target_found)
                                mod_count += 1
                            else:
                                # å…¶ä»–ä¿®é¥°
                                mod_count += 1

                        except:
                            continue

                # è®¾ç½®ä¿®é¥°ä¿¡æ¯
                if modifications:
                    # å¦‚æœæœ‰å¤šä¸ªç›®æ ‡ä¿®é¥°ï¼Œå–ç¬¬ä¸€ä¸ª
                    data.at[idx, 'modification_type'] = modifications[0]
                    data.at[idx, 'num_modifications'] = len(modifications)
                else:
                    # æœ‰ä¿®é¥°ä½†ä¸æ˜¯ç›®æ ‡ä¿®é¥°
                    data.at[idx, 'modification_type'] = 'Other'
                    data.at[idx, 'num_modifications'] = mod_count if mod_count > 0 else 1

        return data
    
    def calculate_properties(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—ç†åŒ–æ€§è´¨"""
        print("âš—ï¸  è®¡ç®—ç†åŒ–æ€§è´¨...")

        # ä¸ºæ¯ä¸ªè‚½æ®µè®¡ç®—ç†åŒ–æ€§è´¨
        properties_list = []

        for idx, row in data.iterrows():
            sequence = row['sequence']

            try:
                if self.properties_analyzer:
                    # ä½¿ç”¨ç°æœ‰çš„åˆ†æå™¨è®¡ç®—ç†åŒ–æ€§è´¨
                    properties = self.properties_analyzer.calculate_peptide_properties(sequence)
                    properties_list.append(properties)
                else:
                    # ä½¿ç”¨ç®€åŒ–çš„ç†åŒ–æ€§è´¨è®¡ç®—
                    properties = self.calculate_simple_properties(sequence)
                    properties_list.append(properties)
            except Exception as e:
                # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨NaN
                properties_list.append({
                    'corrected_pi': np.nan,
                    'corrected_charge_at_ph7': np.nan,
                    'corrected_hydrophobicity_kd': np.nan,
                    'corrected_molecular_weight': np.nan
                })

        # æ·»åŠ ç†åŒ–æ€§è´¨åˆ°æ•°æ®æ¡†
        properties_df = pd.DataFrame(properties_list)
        for col in self.property_columns:
            data[col] = properties_df[col]

        return data

    def calculate_simple_properties(self, sequence: str) -> dict:
        """ç®€åŒ–çš„ç†åŒ–æ€§è´¨è®¡ç®—"""
        # æ°¨åŸºé…¸åŸºæœ¬æ€§è´¨
        aa_properties = {
            'A': {'mw': 71.04, 'pka': 0, 'hydro': 1.8},
            'R': {'mw': 156.10, 'pka': 12.48, 'hydro': -4.5},
            'N': {'mw': 114.04, 'pka': 0, 'hydro': -3.5},
            'D': {'mw': 115.03, 'pka': 3.65, 'hydro': -3.5},
            'C': {'mw': 103.01, 'pka': 8.18, 'hydro': 2.5},
            'E': {'mw': 129.04, 'pka': 4.25, 'hydro': -3.5},
            'Q': {'mw': 128.06, 'pka': 0, 'hydro': -3.5},
            'G': {'mw': 57.02, 'pka': 0, 'hydro': -0.4},
            'H': {'mw': 137.06, 'pka': 6.00, 'hydro': -3.2},
            'I': {'mw': 113.08, 'pka': 0, 'hydro': 4.5},
            'L': {'mw': 113.08, 'pka': 0, 'hydro': 3.8},
            'K': {'mw': 128.09, 'pka': 10.53, 'hydro': -3.9},
            'M': {'mw': 131.04, 'pka': 0, 'hydro': 1.9},
            'F': {'mw': 147.07, 'pka': 0, 'hydro': 2.8},
            'P': {'mw': 97.05, 'pka': 0, 'hydro': -1.6},
            'S': {'mw': 87.03, 'pka': 0, 'hydro': -0.8},
            'T': {'mw': 101.05, 'pka': 0, 'hydro': -0.7},
            'W': {'mw': 186.08, 'pka': 0, 'hydro': -0.9},
            'Y': {'mw': 163.06, 'pka': 10.07, 'hydro': -1.3},
            'V': {'mw': 99.07, 'pka': 0, 'hydro': 4.2}
        }

        # è®¡ç®—åˆ†å­é‡
        mw = 18.015  # æ°´åˆ†å­
        for aa in sequence:
            if aa in aa_properties:
                mw += aa_properties[aa]['mw']

        # è®¡ç®—å‡€ç”µè·ï¼ˆpH 7ï¼‰
        charge = 0
        for aa in sequence:
            if aa in aa_properties:
                pka = aa_properties[aa]['pka']
                if pka > 0:
                    if aa in ['R', 'K', 'H']:  # ç¢±æ€§
                        charge += 1 / (1 + 10**(7 - pka))
                    elif aa in ['D', 'E']:  # é…¸æ€§
                        charge -= 1 / (1 + 10**(pka - 7))

        # Nç«¯å’ŒCç«¯
        charge += 1 / (1 + 10**(7 - 9.6))  # Nç«¯
        charge -= 1 / (1 + 10**(2.34 - 7))  # Cç«¯

        # è®¡ç®—ç–æ°´æ€§ï¼ˆKyte-Doolittleï¼‰
        hydro = 0
        for aa in sequence:
            if aa in aa_properties:
                hydro += aa_properties[aa]['hydro']
        hydro = hydro / len(sequence) if len(sequence) > 0 else 0

        # ç®€åŒ–çš„pIè®¡ç®—ï¼ˆè¿‘ä¼¼ï¼‰
        pi = 7.0 + charge * 2  # ç²—ç•¥ä¼°è®¡

        return {
            'corrected_pi': pi,
            'corrected_charge_at_ph7': charge,
            'corrected_hydrophobicity_kd': hydro,
            'corrected_molecular_weight': mw
        }
    
    def analyze_ptm_effects(self, data: pd.DataFrame, target_ptm: str) -> tuple:
        """åˆ†æç‰¹å®šPTMçš„æ•ˆåº”"""
        print(f"ğŸ”¬ åˆ†æ {target_ptm} çš„æ•ˆåº”...")

        # è°ƒè¯•ï¼šæ£€æŸ¥ä¿®é¥°ç±»å‹åˆ†å¸ƒ
        mod_counts = data['modification_type'].value_counts()
        print(f"   ä¿®é¥°ç±»å‹åˆ†å¸ƒ:")
        for mod_type, count in mod_counts.head(10).items():
            print(f"     {mod_type}: {count}")

        # ç­›é€‰æ•°æ®
        ptm_data = data[data['modification_type'] == target_ptm].copy()
        wt_data = data[data['num_modifications'] == 0].copy()

        if len(ptm_data) == 0:
            print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ° {target_ptm} ä¿®é¥°çš„è‚½æ®µ")
            return None, None

        if len(wt_data) == 0:
            print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœªä¿®é¥°çš„è‚½æ®µ")
            return None, None

        print(f"   {target_ptm} è‚½æ®µ: {len(ptm_data)} æ¡")
        print(f"   æœªä¿®é¥°è‚½æ®µ: {len(wt_data)} æ¡")

        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒ
        ptm_samples = ptm_data['sample_id'].unique()
        wt_samples = wt_data['sample_id'].unique()
        print(f"   {target_ptm} æ ·æœ¬æ•°: {len(ptm_samples)}")
        print(f"   æœªä¿®é¥°æ ·æœ¬æ•°: {len(wt_samples)}")

        # æ£€æŸ¥æ ·æœ¬ç±»å‹åˆ†å¸ƒ
        ptm_sample_types = ptm_data['sample_type'].value_counts()
        wt_sample_types = wt_data['sample_type'].value_counts()
        print(f"   {target_ptm} æ ·æœ¬ç±»å‹: {ptm_sample_types.to_dict()}")
        print(f"   æœªä¿®é¥°æ ·æœ¬ç±»å‹: {wt_sample_types.to_dict()}")
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¸­ä½æ•°
        results = []
        
        for sample_id in data['sample_id'].unique():
            sample_info = data[data['sample_id'] == sample_id].iloc[0]
            sample_type = sample_info['sample_type']
            
            # è¯¥æ ·æœ¬çš„PTMå’ŒWTæ•°æ®
            sample_ptm = ptm_data[ptm_data['sample_id'] == sample_id]
            sample_wt = wt_data[wt_data['sample_id'] == sample_id]
            
            if len(sample_ptm) == 0 or len(sample_wt) == 0:
                continue
            
            # è®¡ç®—æ¯ä¸ªç†åŒ–æ€§è´¨çš„ratio
            for prop_col, prop_name in zip(self.property_columns, self.property_names):
                ptm_median = sample_ptm[prop_col].median()
                wt_median = sample_wt[prop_col].median()
                
                if pd.notna(ptm_median) and pd.notna(wt_median) and wt_median != 0:
                    ratio = ptm_median / wt_median
                    log2_ratio = np.log2(ratio)
                    
                    results.append({
                        'sample_id': sample_id,
                        'sample_type': sample_type,
                        'property': prop_name,
                        'ptm_median': ptm_median,
                        'wt_median': wt_median,
                        'ratio': ratio,
                        'log2_ratio': log2_ratio,
                        'ptm_count': len(sample_ptm),
                        'wt_count': len(sample_wt)
                    })
        
        if not results:
            print(f"âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„ratioè®¡ç®—ç»“æœ")
            return None, None
        
        summary_df = pd.DataFrame(results)
        
        # ç»Ÿè®¡æ£€éªŒ
        stats_results = self.perform_statistical_tests(summary_df, target_ptm)
        
        return summary_df, stats_results
    
    def perform_statistical_tests(self, summary_df: pd.DataFrame, target_ptm: str) -> pd.DataFrame:
        """æ‰§è¡Œç»Ÿè®¡æ£€éªŒ"""
        print(f"ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡æ£€éªŒ...")
        
        stats_results = []
        
        for prop_name in self.property_names:
            prop_data = summary_df[summary_df['property'] == prop_name]
            
            if len(prop_data) == 0:
                continue
            
            tumor_data = prop_data[prop_data['sample_type'] == 'Tumor']['log2_ratio']
            normal_data = prop_data[prop_data['sample_type'] == 'Normal']['log2_ratio']
            
            if len(tumor_data) == 0 or len(normal_data) == 0:
                continue
            
            # Mann-Whitney Uæ£€éªŒï¼ˆéé…å¯¹ï¼‰
            try:
                statistic, p_value = stats.mannwhitneyu(
                    tumor_data, normal_data, alternative='two-sided'
                )
                
                # è®¡ç®—æ•ˆåº”é‡ï¼ˆCliff's deltaï¼‰
                cliff_delta = self.calculate_cliff_delta(tumor_data, normal_data)
                
                stats_results.append({
                    'ptm': target_ptm,
                    'property': prop_name,
                    'test_type': 'Mann-Whitney U',
                    'statistic': statistic,
                    'p_value': p_value,
                    'cliff_delta': cliff_delta,
                    'tumor_n': len(tumor_data),
                    'normal_n': len(normal_data),
                    'tumor_median': tumor_data.median(),
                    'normal_median': normal_data.median(),
                    'tumor_mean': tumor_data.mean(),
                    'normal_mean': normal_data.mean()
                })
                
            except Exception as e:
                print(f"âš ï¸  ç»Ÿè®¡æ£€éªŒå¤±è´¥ {prop_name}: {e}")
                continue
        
        if not stats_results:
            return pd.DataFrame()
        
        stats_df = pd.DataFrame(stats_results)
        
        # å¤šé‡æ¯”è¾ƒæ ¡æ­£
        if len(stats_df) > 1:
            if HAS_STATSMODELS:
                _, p_adj, _, _ = multipletests(stats_df['p_value'], method='fdr_bh')
                stats_df['p_adj'] = p_adj
            else:
                # ç®€å•çš„Bonferroniæ ¡æ­£
                stats_df['p_adj'] = stats_df['p_value'] * len(stats_df)
                stats_df['p_adj'] = np.minimum(stats_df['p_adj'], 1.0)
        else:
            stats_df['p_adj'] = stats_df['p_value']
        
        # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
        stats_df['significant'] = stats_df['p_adj'] < 0.05
        
        return stats_df
    
    def calculate_cliff_delta(self, x, y):
        """è®¡ç®—Cliff's deltaæ•ˆåº”é‡"""
        try:
            n1, n2 = len(x), len(y)
            if n1 == 0 or n2 == 0:
                return np.nan
            
            # è®¡ç®—æ‰€æœ‰é…å¯¹æ¯”è¾ƒ
            comparisons = []
            for xi in x:
                for yi in y:
                    if xi > yi:
                        comparisons.append(1)
                    elif xi < yi:
                        comparisons.append(-1)
                    else:
                        comparisons.append(0)
            
            cliff_delta = np.mean(comparisons)
            return cliff_delta
            
        except:
            return np.nan

    def create_visualizations(self, summary_df: pd.DataFrame, stats_df: pd.DataFrame, target_ptm: str):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print(f"ğŸ“Š åˆ›å»º {target_ptm} çš„å¯è§†åŒ–å›¾è¡¨...")

        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")

        # è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥é¿å…ä¹±ç 
        plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False

        # ä¸ºæ¯ä¸ªç†åŒ–æ€§è´¨åˆ›å»ºå›¾è¡¨
        for prop_name in self.property_names:
            prop_data = summary_df[summary_df['property'] == prop_name]

            if len(prop_data) == 0:
                continue

            # åˆ›å»ºå­å›¾
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))

            # 1. ç®±çº¿å›¾/å°æç´å›¾
            ax1 = axes[0]

            # å°æç´å›¾
            sns.violinplot(data=prop_data, x='sample_type', y='log2_ratio',
                          ax=ax1, inner='box', width=0.6)

            # æ·»åŠ æ•£ç‚¹
            sns.stripplot(data=prop_data, x='sample_type', y='log2_ratio',
                         ax=ax1, size=4, alpha=0.7, color='black')

            ax1.set_title(f'{target_ptm} - {prop_name}\nlog2(PTM/WT ratio)')
            ax1.set_xlabel('Sample Type')
            ax1.set_ylabel('log2(PTM/WT ratio)')
            ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            prop_stats = stats_df[stats_df['property'] == prop_name]
            if len(prop_stats) > 0:
                stat_info = prop_stats.iloc[0]
                p_val = stat_info['p_adj']

                if p_val < 0.001:
                    sig_text = '***'
                elif p_val < 0.01:
                    sig_text = '**'
                elif p_val < 0.05:
                    sig_text = '*'
                else:
                    sig_text = 'ns'

                ax1.text(0.5, 0.95, f'p = {p_val:.3f} {sig_text}',
                        transform=ax1.transAxes, ha='center', va='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            # 2. é…å¯¹è¿çº¿å›¾ï¼ˆå¦‚æœæœ‰é…å¯¹æ ·æœ¬ï¼‰
            ax2 = axes[1]

            # æ£€æŸ¥æ˜¯å¦æœ‰é…å¯¹æ ·æœ¬
            tumor_samples = set(prop_data[prop_data['sample_type'] == 'Tumor']['sample_id'])
            normal_samples = set(prop_data[prop_data['sample_type'] == 'Normal']['sample_id'])

            # ç®€åŒ–çš„é…å¯¹é€»è¾‘ï¼šåŸºäºæ ·æœ¬åç§°ç›¸ä¼¼æ€§
            paired_data = []
            for tumor_sample in tumor_samples:
                # å¯»æ‰¾æœ€ç›¸ä¼¼çš„normalæ ·æœ¬
                best_match = None
                best_score = 0

                for normal_sample in normal_samples:
                    # è®¡ç®—æ ·æœ¬åç§°ç›¸ä¼¼æ€§
                    common_parts = set(tumor_sample.split('_')) & set(normal_sample.split('_'))
                    score = len(common_parts)

                    if score > best_score:
                        best_score = score
                        best_match = normal_sample

                if best_match and best_score >= 2:  # è‡³å°‘æœ‰2ä¸ªå…±åŒéƒ¨åˆ†
                    tumor_value = prop_data[
                        (prop_data['sample_id'] == tumor_sample) &
                        (prop_data['sample_type'] == 'Tumor')
                    ]['log2_ratio'].iloc[0]

                    normal_value = prop_data[
                        (prop_data['sample_id'] == best_match) &
                        (prop_data['sample_type'] == 'Normal')
                    ]['log2_ratio'].iloc[0]

                    paired_data.append({
                        'tumor_sample': tumor_sample,
                        'normal_sample': best_match,
                        'tumor_value': tumor_value,
                        'normal_value': normal_value
                    })

            if paired_data:
                # ç»˜åˆ¶é…å¯¹è¿çº¿å›¾
                paired_df = pd.DataFrame(paired_data)

                for _, row in paired_df.iterrows():
                    ax2.plot([0, 1], [row['normal_value'], row['tumor_value']],
                            'o-', alpha=0.6, linewidth=1)

                ax2.set_xlim(-0.2, 1.2)
                ax2.set_xticks([0, 1])
                ax2.set_xticklabels(['Normal', 'Tumor'])
                ax2.set_ylabel('log2(PTM/WT ratio)')
                ax2.set_title(f'Paired Samples (n={len(paired_df)})')
                ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)

                # é…å¯¹tæ£€éªŒ
                if len(paired_df) >= 3:
                    try:
                        t_stat, t_p = stats.ttest_rel(paired_df['tumor_value'],
                                                     paired_df['normal_value'])
                        ax2.text(0.5, 0.95, f'Paired t-test: p = {t_p:.3f}',
                                transform=ax2.transAxes, ha='center', va='top',
                                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                    except:
                        pass
            else:
                # å¦‚æœæ²¡æœ‰é…å¯¹æ ·æœ¬ï¼Œæ˜¾ç¤ºåˆ†ç»„ç®±çº¿å›¾
                sns.boxplot(data=prop_data, x='sample_type', y='log2_ratio', ax=ax2)
                ax2.set_title('Unpaired Comparison')
                ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)

            plt.tight_layout()

            # ä¿å­˜å›¾è¡¨
            filename = f'{target_ptm}_{prop_name}_comparison.png'
            filepath = os.path.join(self.output_dir, filename)
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"   ä¿å­˜å›¾è¡¨: {filename}")

    def save_results(self, summary_df: pd.DataFrame, stats_df: pd.DataFrame, target_ptm: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        print(f"ğŸ’¾ ä¿å­˜ {target_ptm} çš„åˆ†æç»“æœ...")

        # ä¿å­˜summaryæ•°æ®
        summary_file = os.path.join(self.output_dir, f'summary_{target_ptm}.csv')
        summary_df.to_csv(summary_file, index=False)

        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_file = os.path.join(self.output_dir, f'stats_{target_ptm}.txt')
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(f"Statistical Analysis Results for {target_ptm}\n")
            f.write("=" * 50 + "\n\n")

            for _, row in stats_df.iterrows():
                f.write(f"Property: {row['property']}\n")
                f.write(f"Test: {row['test_type']}\n")
                f.write(f"Statistic: {row['statistic']:.4f}\n")
                f.write(f"P-value: {row['p_value']:.6f}\n")
                f.write(f"Adjusted P-value: {row['p_adj']:.6f}\n")
                f.write(f"Significant: {'Yes' if row['significant'] else 'No'}\n")
                f.write(f"Cliff's Delta: {row['cliff_delta']:.4f}\n")
                f.write(f"Tumor median: {row['tumor_median']:.4f}\n")
                f.write(f"Normal median: {row['normal_median']:.4f}\n")
                f.write(f"Sample sizes: Tumor={row['tumor_n']}, Normal={row['normal_n']}\n")
                f.write("-" * 30 + "\n\n")

        print(f"   ä¿å­˜æ•°æ®: summary_{target_ptm}.csv")
        print(f"   ä¿å­˜ç»Ÿè®¡: stats_{target_ptm}.txt")

    def analyze_dataset(self, dataset_id: str, target_ptms: list = None):
        """åˆ†æå•ä¸ªæ•°æ®é›†"""
        if target_ptms is None:
            target_ptms = self.target_modifications

        print(f"\nğŸ”¬ å¼€å§‹åˆ†ææ•°æ®é›†: {dataset_id}")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        data = self.load_and_prepare_data(dataset_id)
        if data is None:
            return

        # åˆ†ææ¯ç§PTM
        all_results = {}

        for ptm in target_ptms:
            print(f"\nğŸ“Š åˆ†æ {ptm}...")

            summary_df, stats_df = self.analyze_ptm_effects(data, ptm)

            if summary_df is not None and stats_df is not None:
                # åˆ›å»ºå¯è§†åŒ–
                self.create_visualizations(summary_df, stats_df, ptm)

                # ä¿å­˜ç»“æœ
                self.save_results(summary_df, stats_df, ptm)

                all_results[ptm] = {
                    'summary': summary_df,
                    'stats': stats_df
                }

                print(f"âœ… {ptm} åˆ†æå®Œæˆ")
            else:
                print(f"âŒ {ptm} åˆ†æå¤±è´¥")

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report(all_results, dataset_id)

        print(f"\nğŸ‰ æ•°æ®é›† {dataset_id} åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")

    def generate_comprehensive_report(self, all_results: dict, dataset_id: str):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")

        report_file = os.path.join(self.output_dir, f'comprehensive_report_{dataset_id}.txt')

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"Tumor vs Normal PTM Analysis Report\n")
            f.write(f"Dataset: {dataset_id}\n")
            f.write("=" * 60 + "\n\n")

            f.write("SUMMARY\n")
            f.write("-" * 20 + "\n")
            f.write(f"Analyzed PTMs: {len(all_results)}\n")
            f.write(f"Analysis method: logâ‚‚(PTM median / WT median) ratio comparison\n")
            f.write(f"Statistical test: Mann-Whitney U test\n")
            f.write(f"Multiple testing correction: FDR (Benjamini-Hochberg)\n\n")

            # æ˜¾è‘—æ€§ç»“æœæ±‡æ€»
            f.write("SIGNIFICANT RESULTS\n")
            f.write("-" * 20 + "\n")

            significant_count = 0
            for ptm, results in all_results.items():
                stats_df = results['stats']
                significant_results = stats_df[stats_df['significant']]

                if len(significant_results) > 0:
                    f.write(f"\n{ptm}:\n")
                    for _, row in significant_results.iterrows():
                        direction = "â†‘" if row['tumor_median'] > row['normal_median'] else "â†“"
                        f.write(f"  {row['property']}: p_adj = {row['p_adj']:.4f} {direction}\n")
                        f.write(f"    Tumor median: {row['tumor_median']:.4f}\n")
                        f.write(f"    Normal median: {row['normal_median']:.4f}\n")
                        f.write(f"    Effect size (Cliff's Î´): {row['cliff_delta']:.4f}\n")
                    significant_count += len(significant_results)

            if significant_count == 0:
                f.write("No significant differences found.\n")

            f.write(f"\nTotal significant comparisons: {significant_count}\n")

        print(f"   ä¿å­˜æŠ¥å‘Š: comprehensive_report_{dataset_id}.txt")

    def find_tumor_normal_datasets(self):
        """æŸ¥æ‰¾åŒ…å«Tumorå’ŒNormalæ ·æœ¬çš„æ•°æ®é›†"""
        print("ğŸ” æŸ¥æ‰¾åŒ…å«Tumorå’ŒNormalæ ·æœ¬çš„æ•°æ®é›†...")

        tumor_normal_datasets = []

        # æ£€æŸ¥tumor_summaryç›®å½•ä¸­çš„æ‰€æœ‰summaryæ–‡ä»¶
        summary_dir = 'tumor_summary'
        if not os.path.exists(summary_dir):
            print(f"âŒ æ‰¾ä¸åˆ°summaryç›®å½•: {summary_dir}")
            return []

        for file in os.listdir(summary_dir):
            if file.endswith('_summary.csv'):
                dataset_id = file.replace('_summary.csv', '')
                summary_file = os.path.join(summary_dir, file)

                try:
                    df = pd.read_csv(summary_file)
                    if 'Type' in df.columns:
                        types = df['Type'].unique()
                        has_both = 'Tumor' in types and 'Normal' in types

                        if has_both:
                            tumor_count = len(df[df['Type'] == 'Tumor'])
                            normal_count = len(df[df['Type'] == 'Normal'])

                            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æ•°æ®ç›®å½•
                            data_dir = f'{dataset_id}_human'
                            if os.path.exists(data_dir):
                                tumor_normal_datasets.append({
                                    'dataset_id': dataset_id,
                                    'tumor_count': tumor_count,
                                    'normal_count': normal_count
                                })
                                print(f"   âœ… {dataset_id}: Tumor={tumor_count}, Normal={normal_count}")
                            else:
                                print(f"   âš ï¸  {dataset_id}: æœ‰summaryä½†ç¼ºå°‘æ•°æ®ç›®å½•")
                except Exception as e:
                    print(f"   âŒ {dataset_id}: è¯»å–summaryå¤±è´¥ - {e}")

        print(f"\nğŸ“Š æ€»å…±æ‰¾åˆ° {len(tumor_normal_datasets)} ä¸ªå¯åˆ†æçš„æ•°æ®é›†")
        return tumor_normal_datasets

    def check_dataset_completion(self, dataset_id: str, target_ptms: list) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ç»å®Œæˆåˆ†æ"""
        dataset_output_dir = f'tumor_vs_normal_ptm_results/{dataset_id}'

        if not os.path.exists(dataset_output_dir):
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰ç»¼åˆæŠ¥å‘Š
        report_file = os.path.join(dataset_output_dir, f'comprehensive_report_{dataset_id}.txt')
        if not os.path.exists(report_file):
            return False

        # æ£€æŸ¥æ¯ä¸ªPTMçš„ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for ptm in target_ptms:
            summary_file = os.path.join(dataset_output_dir, f'summary_{ptm}.csv')
            stats_file = os.path.join(dataset_output_dir, f'stats_{ptm}.txt')

            if not (os.path.exists(summary_file) and os.path.exists(stats_file)):
                return False

        return True

    def save_progress(self, completed_datasets: list, failed_datasets: list):
        """ä¿å­˜åˆ†æè¿›åº¦"""
        progress_file = os.path.join(self.output_dir, 'analysis_progress.json')

        progress_data = {
            'completed_datasets': completed_datasets,
            'failed_datasets': failed_datasets,
            'timestamp': pd.Timestamp.now().isoformat()
        }

        import json
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False)

    def load_progress(self):
        """åŠ è½½åˆ†æè¿›åº¦"""
        progress_file = os.path.join(self.output_dir, 'analysis_progress.json')

        if not os.path.exists(progress_file):
            return [], []

        try:
            import json
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)

            return progress_data.get('completed_datasets', []), progress_data.get('failed_datasets', [])
        except Exception as e:
            print(f"âš ï¸  è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return [], []

    def analyze_all_datasets(self, target_ptms: list = None, resume: bool = True):
        """åˆ†ææ‰€æœ‰åŒ…å«Tumorå’ŒNormalæ ·æœ¬çš„æ•°æ®é›†"""
        if target_ptms is None:
            target_ptms = ['Phospho[S]', 'Acetyl[K]', 'Deamidated[N]']

        # æŸ¥æ‰¾æ‰€æœ‰å¯åˆ†æçš„æ•°æ®é›†
        datasets = self.find_tumor_normal_datasets()

        if not datasets:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«Tumorå’ŒNormalæ ·æœ¬çš„æ•°æ®é›†")
            return

        # åŠ è½½ä¹‹å‰çš„è¿›åº¦
        completed_datasets, failed_datasets = [], []
        if resume:
            completed_datasets, failed_datasets = self.load_progress()

            if completed_datasets or failed_datasets:
                print(f"\nï¿½ æ£€æµ‹åˆ°ä¹‹å‰çš„åˆ†æè¿›åº¦:")
                print(f"   å·²å®Œæˆ: {len(completed_datasets)} ä¸ªæ•°æ®é›†")
                print(f"   å·²å¤±è´¥: {len(failed_datasets)} ä¸ªæ•°æ®é›†")

        # æ£€æŸ¥æ¯ä¸ªæ•°æ®é›†çš„å®ŒæˆçŠ¶æ€
        datasets_to_analyze = []
        for dataset_info in datasets:
            dataset_id = dataset_info['dataset_id']

            if dataset_id in completed_datasets:
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨
                if self.check_dataset_completion(dataset_id, target_ptms):
                    print(f"   âœ… {dataset_id}: å·²å®Œæˆï¼Œè·³è¿‡")
                    continue
                else:
                    print(f"   âš ï¸  {dataset_id}: æ ‡è®°ä¸ºå®Œæˆä½†æ–‡ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°åˆ†æ")
                    completed_datasets.remove(dataset_id)

            datasets_to_analyze.append(dataset_info)

        if not datasets_to_analyze:
            print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†éƒ½å·²å®Œæˆåˆ†æï¼")
            self.generate_overall_report(completed_datasets, failed_datasets, datasets)
            return

        print(f"\nğŸš€ å¼€å§‹åˆ†æ {len(datasets_to_analyze)} ä¸ªæ•°æ®é›†...")
        print("=" * 60)

        for i, dataset_info in enumerate(datasets_to_analyze, 1):
            dataset_id = dataset_info['dataset_id']

            print(f"\nğŸ“Š [{i}/{len(datasets_to_analyze)}] åˆ†ææ•°æ®é›†: {dataset_id}")
            print(f"   Tumoræ ·æœ¬: {dataset_info['tumor_count']}")
            print(f"   Normalæ ·æœ¬: {dataset_info['normal_count']}")

            try:
                # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºå•ç‹¬çš„è¾“å‡ºç›®å½•
                original_output_dir = self.output_dir
                self.output_dir = f'tumor_vs_normal_ptm_results/{dataset_id}'
                os.makedirs(self.output_dir, exist_ok=True)

                # åˆ†ææ•°æ®é›†
                self.analyze_dataset(dataset_id, target_ptms)
                completed_datasets.append(dataset_id)

                # æ¢å¤åŸå§‹è¾“å‡ºç›®å½•
                self.output_dir = original_output_dir

                # ä¿å­˜è¿›åº¦
                self.save_progress(completed_datasets, failed_datasets)

                print(f"   âœ… {dataset_id} åˆ†æå®Œæˆå¹¶ä¿å­˜è¿›åº¦")

            except Exception as e:
                print(f"âŒ æ•°æ®é›† {dataset_id} åˆ†æå¤±è´¥: {e}")
                failed_datasets.append((dataset_id, str(e)))

                # æ¢å¤åŸå§‹è¾“å‡ºç›®å½•
                self.output_dir = original_output_dir

                # ä¿å­˜è¿›åº¦
                self.save_progress(completed_datasets, failed_datasets)
                continue

        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        self.generate_overall_report(completed_datasets, failed_datasets, datasets)

        # ç”Ÿæˆæ•°æ®é›†çº§åˆ«æ±‡æ€»åˆ†æ
        if completed_datasets:
            self.create_dataset_level_summary(completed_datasets)

        print(f"\nğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼")
        print(f"   æˆåŠŸ: {len(completed_datasets)} ä¸ªæ•°æ®é›†")
        print(f"   å¤±è´¥: {len(failed_datasets)} ä¸ªæ•°æ®é›†")
        print(f"   æ•°æ®é›†çº§åˆ«æ±‡æ€»: dataset_level_summary.csv")
        print(f"   æ±‡æ€»å¯è§†åŒ–: dataset_level_*.png")

    def generate_overall_report(self, successful_analyses, failed_analyses, all_datasets):
        """ç”Ÿæˆæ€»ä½“åˆ†ææŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆæ€»ä½“åˆ†ææŠ¥å‘Š...")

        report_file = os.path.join(self.output_dir, 'overall_tumor_vs_normal_report.txt')

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Tumor vs Normal PTM Analysis - Overall Report\n")
            f.write("=" * 60 + "\n\n")

            f.write("SUMMARY\n")
            f.write("-" * 20 + "\n")
            f.write(f"Total datasets analyzed: {len(successful_analyses)}\n")
            f.write(f"Failed analyses: {len(failed_analyses)}\n")
            f.write(f"Analysis method: logâ‚‚(PTM median / WT median) ratio comparison\n")
            f.write(f"Statistical test: Mann-Whitney U test\n")
            f.write(f"PTMs analyzed: {', '.join(self.target_modifications[:3])}\n\n")

            f.write("SUCCESSFUL ANALYSES\n")
            f.write("-" * 20 + "\n")
            for dataset_id in successful_analyses:
                dataset_info = next(d for d in all_datasets if d['dataset_id'] == dataset_id)
                f.write(f"{dataset_id}: Tumor={dataset_info['tumor_count']}, Normal={dataset_info['normal_count']}\n")

            if failed_analyses:
                f.write(f"\nFAILED ANALYSES\n")
                f.write("-" * 20 + "\n")
                for dataset_id, error in failed_analyses:
                    f.write(f"{dataset_id}: {error}\n")

            f.write(f"\nRESULTS LOCATION\n")
            f.write("-" * 20 + "\n")
            f.write(f"Individual results: tumor_vs_normal_ptm_results/[DATASET_ID]/\n")
            f.write(f"Overall report: {report_file}\n")

        print(f"   ä¿å­˜æ€»ä½“æŠ¥å‘Š: overall_tumor_vs_normal_report.txt")

    def create_dataset_level_summary(self, successful_analyses: list):
        """åˆ›å»ºæ•°æ®é›†çº§åˆ«çš„æ±‡æ€»åˆ†æ"""
        print("ğŸ“Š åˆ›å»ºæ•°æ®é›†çº§åˆ«çš„æ±‡æ€»åˆ†æ...")

        # æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„ç»“æœ
        all_dataset_results = []
        target_ptms = ['Phospho[S]', 'Acetyl[K]', 'Deamidated[N]']

        for dataset_id in successful_analyses:
            dataset_dir = f'tumor_vs_normal_ptm_results/{dataset_id}'

            for ptm in target_ptms:
                stats_file = os.path.join(dataset_dir, f'stats_{ptm}.txt')
                summary_file = os.path.join(dataset_dir, f'summary_{ptm}.csv')

                if os.path.exists(stats_file) and os.path.exists(summary_file):
                    # è¯»å–ç»Ÿè®¡ç»“æœ
                    stats_data = self.parse_stats_file(stats_file, dataset_id, ptm)

                    # è¯»å–æ ·æœ¬æ•°é‡ä¿¡æ¯
                    summary_df = pd.read_csv(summary_file)
                    sample_counts = summary_df.groupby('sample_type').size()

                    for stat_result in stats_data:
                        stat_result.update({
                            'dataset_id': dataset_id,
                            'ptm': ptm,
                            'tumor_samples': sample_counts.get('Tumor', 0),
                            'normal_samples': sample_counts.get('Normal', 0)
                        })
                        all_dataset_results.append(stat_result)

        if not all_dataset_results:
            print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯æ±‡æ€»çš„ç»“æœ")
            return

        # åˆ›å»ºæ±‡æ€»æ•°æ®æ¡†
        summary_df = pd.DataFrame(all_dataset_results)

        # ä¿å­˜æ•°æ®é›†çº§åˆ«æ±‡æ€»
        summary_file = os.path.join(self.output_dir, 'dataset_level_summary.csv')
        summary_df.to_csv(summary_file, index=False)

        # åˆ›å»ºæ±‡æ€»å¯è§†åŒ–
        self.create_dataset_level_visualizations(summary_df)

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_dataset_level_report(summary_df)

        print(f"   ä¿å­˜æ•°æ®é›†æ±‡æ€»: dataset_level_summary.csv")

    def parse_stats_file(self, stats_file: str, dataset_id: str, ptm: str) -> list:
        """è§£æç»Ÿè®¡ç»“æœæ–‡ä»¶"""
        results = []

        try:
            with open(stats_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æŒ‰åˆ†éš”çº¿åˆ†å‰²æ¯ä¸ªpropertyçš„ç»“æœ
            sections = content.split('------------------------------')

            for section in sections:
                lines = [line.strip() for line in section.strip().split('\n') if line.strip()]

                if len(lines) < 5:  # è‡³å°‘éœ€è¦å‡ ä¸ªå…³é”®å­—æ®µ
                    continue

                result = {'dataset_id': dataset_id, 'ptm': ptm}

                # è§£æå„ä¸ªå­—æ®µ
                for line in lines:
                    if ':' not in line or line.startswith('=') or line.startswith('Statistical'):
                        continue

                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()

                    if key == 'Property':
                        result['property'] = value
                    elif key == 'P-value':
                        try:
                            if value.lower() == 'nan':
                                result['p_value'] = None
                            else:
                                result['p_value'] = float(value)
                        except:
                            result['p_value'] = None
                    elif key == 'Adjusted P-value':
                        try:
                            if value.lower() == 'nan':
                                result['p_adj'] = None
                            else:
                                result['p_adj'] = float(value)
                        except:
                            result['p_adj'] = None
                    elif key == 'Significant':
                        result['significant'] = 'Yes' in value
                    elif key == "Cliff's Delta":
                        try:
                            if value.lower() == 'nan':
                                result['cliff_delta'] = None
                            else:
                                result['cliff_delta'] = float(value)
                        except:
                            result['cliff_delta'] = None
                    elif key == 'Tumor median':
                        try:
                            if value.lower() == 'nan':
                                result['tumor_median'] = None
                            else:
                                result['tumor_median'] = float(value)
                        except:
                            result['tumor_median'] = None
                    elif key == 'Normal median':
                        try:
                            if value.lower() == 'nan':
                                result['normal_median'] = None
                            else:
                                result['normal_median'] = float(value)
                        except:
                            result['normal_median'] = None
                    elif key == 'Sample sizes':
                        # è§£ææ ·æœ¬é‡ä¿¡æ¯: "Tumor=17, Normal=7"
                        if 'Tumor=' in value and 'Normal=' in value:
                            try:
                                tumor_part = value.split('Tumor=')[1].split(',')[0]
                                normal_part = value.split('Normal=')[1]
                                result['tumor_n'] = int(tumor_part.strip())
                                result['normal_n'] = int(normal_part.strip())
                            except:
                                pass

                # åªæœ‰åŒ…å«propertyçš„è®°å½•æ‰æ·»åŠ 
                if 'property' in result:
                    results.append(result)

        except Exception as e:
            print(f"   âš ï¸  è§£æç»Ÿè®¡æ–‡ä»¶å¤±è´¥ {stats_file}: {e}")

        return results

    def create_dataset_level_visualizations(self, summary_df: pd.DataFrame):
        """åˆ›å»ºæ•°æ®é›†çº§åˆ«çš„å¯è§†åŒ–"""
        print("   åˆ›å»ºæ•°æ®é›†çº§åˆ«å¯è§†åŒ–...")

        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False

        # 1. æ˜¾è‘—æ€§ç»“æœçƒ­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        ptms = ['Phospho[S]', 'Acetyl[K]', 'Deamidated[N]']
        properties = ['pI', 'Net_Charge', 'Hydrophobicity_KD', 'Molecular_Weight']

        for i, ptm in enumerate(ptms):
            if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªPTM
                break

            ax = axes[i//2, i%2] if i < 2 else axes[1, 0]

            ptm_data = summary_df[summary_df['ptm'] == ptm]

            if len(ptm_data) == 0:
                ax.text(0.5, 0.5, f'No data for {ptm}', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{ptm} - No Data')
                continue

            # åˆ›å»ºçƒ­å›¾æ•°æ®
            heatmap_data = ptm_data.pivot_table(
                index='dataset_id',
                columns='property',
                values='p_adj',
                fill_value=1.0
            )

            # åˆ›å»ºæ˜¾è‘—æ€§æ ‡è®°
            sig_data = ptm_data.pivot_table(
                index='dataset_id',
                columns='property',
                values='significant',
                fill_value=False
            )

            # ç»˜åˆ¶çƒ­å›¾
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlBu_r',
                       ax=ax, cbar_kws={'label': 'Adjusted P-value'})

            # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
            for y, dataset in enumerate(heatmap_data.index):
                for x, prop in enumerate(heatmap_data.columns):
                    if prop in sig_data.columns and dataset in sig_data.index:
                        if sig_data.loc[dataset, prop]:
                            ax.text(x+0.5, y+0.5, '*', ha='center', va='center',
                                   color='white', fontsize=16, fontweight='bold')

            ax.set_title(f'{ptm} Significance Across Datasets')
            ax.set_xlabel('Physicochemical Properties')
            ax.set_ylabel('Dataset ID')

        # éšè—ç¬¬4ä¸ªå­å›¾
        axes[1, 1].axis('off')

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'dataset_level_significance_heatmap.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        # 2. æ•ˆåº”é‡åˆ†å¸ƒå›¾
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        for i, ptm in enumerate(ptms):
            ptm_data = summary_df[summary_df['ptm'] == ptm]

            if len(ptm_data) == 0:
                continue

            # è®¡ç®—log2 fold change
            ptm_data = ptm_data.copy()
            ptm_data['log2_fc'] = ptm_data.apply(
                lambda row: np.log2(row['tumor_median'] / row['normal_median'])
                if pd.notna(row['tumor_median']) and pd.notna(row['normal_median']) and row['normal_median'] != 0
                else np.nan, axis=1
            )

            # æŒ‰propertyåˆ†ç»„ç»˜åˆ¶
            for prop in properties:
                prop_data = ptm_data[ptm_data['property'] == prop]
                if len(prop_data) > 0:
                    x_pos = [j + i*0.2 for j in range(len(prop_data))]
                    colors = ['red' if sig else 'gray' for sig in prop_data['significant']]

                    axes[i].scatter(x_pos, prop_data['log2_fc'],
                                  c=colors, alpha=0.7, s=60, label=prop)

            axes[i].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            axes[i].set_title(f'{ptm} Effect Sizes')
            axes[i].set_xlabel('Datasets')
            axes[i].set_ylabel('log2(Tumor/Normal)')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'dataset_level_effect_sizes.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        # 3. æ ·æœ¬é‡åˆ†å¸ƒå›¾
        plt.figure(figsize=(12, 8))

        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„æ€»æ ·æœ¬é‡
        dataset_samples = summary_df.groupby('dataset_id').agg({
            'tumor_samples': 'first',
            'normal_samples': 'first'
        }).reset_index()

        dataset_samples['total_samples'] = dataset_samples['tumor_samples'] + dataset_samples['normal_samples']
        dataset_samples = dataset_samples.sort_values('total_samples')

        # åˆ›å»ºå †å æ¡å½¢å›¾
        x_pos = range(len(dataset_samples))
        plt.bar(x_pos, dataset_samples['tumor_samples'], label='Tumor Samples', color='red', alpha=0.7)
        plt.bar(x_pos, dataset_samples['normal_samples'],
               bottom=dataset_samples['tumor_samples'], label='Normal Samples', color='blue', alpha=0.7)

        plt.xlabel('Dataset ID')
        plt.ylabel('Number of Samples')
        plt.title('Sample Distribution Across Datasets')
        plt.xticks(x_pos, dataset_samples['dataset_id'], rotation=45, ha='right')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # æ·»åŠ æ€»æ•°æ ‡ç­¾
        for i, (_, row) in enumerate(dataset_samples.iterrows()):
            plt.text(i, row['total_samples'] + 5, str(row['total_samples']),
                    ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'dataset_sample_distribution.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

        print(f"      ä¿å­˜å¯è§†åŒ–: dataset_level_significance_heatmap.png")
        print(f"      ä¿å­˜å¯è§†åŒ–: dataset_level_effect_sizes.png")
        print(f"      ä¿å­˜å¯è§†åŒ–: dataset_sample_distribution.png")

    def generate_dataset_level_report(self, summary_df: pd.DataFrame):
        """ç”Ÿæˆæ•°æ®é›†çº§åˆ«çš„åˆ†ææŠ¥å‘Š"""
        print("   ç”Ÿæˆæ•°æ®é›†çº§åˆ«æŠ¥å‘Š...")

        report_file = os.path.join(self.output_dir, 'dataset_level_analysis_report.txt')

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Dataset-Level Tumor vs Normal PTM Analysis Report\n")
            f.write("=" * 60 + "\n\n")

            # æ€»ä½“ç»Ÿè®¡
            f.write("OVERALL STATISTICS\n")
            f.write("-" * 20 + "\n")
            f.write(f"Total datasets analyzed: {summary_df['dataset_id'].nunique()}\n")
            f.write(f"Total PTMs analyzed: {summary_df['ptm'].nunique()}\n")
            f.write(f"Total comparisons: {len(summary_df)}\n")
            f.write(f"Significant comparisons: {summary_df['significant'].sum()}\n")
            f.write(f"Significance rate: {summary_df['significant'].mean()*100:.1f}%\n\n")

            # æŒ‰PTMç»Ÿè®¡
            f.write("PTM-SPECIFIC RESULTS\n")
            f.write("-" * 20 + "\n")

            for ptm in summary_df['ptm'].unique():
                ptm_data = summary_df[summary_df['ptm'] == ptm]
                sig_count = ptm_data['significant'].sum()
                total_count = len(ptm_data)

                f.write(f"\n{ptm}:\n")
                f.write(f"  Total comparisons: {total_count}\n")
                f.write(f"  Significant results: {sig_count}\n")
                f.write(f"  Significance rate: {sig_count/total_count*100:.1f}%\n")

                # æŒ‰propertyç»†åˆ†
                for prop in ptm_data['property'].unique():
                    prop_data = ptm_data[ptm_data['property'] == prop]
                    prop_sig = prop_data['significant'].sum()
                    prop_total = len(prop_data)

                    f.write(f"    {prop}: {prop_sig}/{prop_total} ({prop_sig/prop_total*100:.1f}%)\n")

            # æŒ‰æ•°æ®é›†ç»Ÿè®¡
            f.write(f"\nDATASET-SPECIFIC RESULTS\n")
            f.write("-" * 20 + "\n")

            for dataset in summary_df['dataset_id'].unique():
                dataset_data = summary_df[summary_df['dataset_id'] == dataset]
                sig_count = dataset_data['significant'].sum()
                total_count = len(dataset_data)

                # è·å–æ ·æœ¬é‡ä¿¡æ¯
                tumor_samples = dataset_data['tumor_samples'].iloc[0] if len(dataset_data) > 0 else 0
                normal_samples = dataset_data['normal_samples'].iloc[0] if len(dataset_data) > 0 else 0

                f.write(f"\n{dataset}:\n")
                f.write(f"  Sample sizes: Tumor={tumor_samples}, Normal={normal_samples}\n")
                f.write(f"  Total comparisons: {total_count}\n")
                f.write(f"  Significant results: {sig_count}\n")
                f.write(f"  Significance rate: {sig_count/total_count*100:.1f}%\n")

                # æ˜¾è‘—æ€§ç»“æœè¯¦æƒ…
                sig_results = dataset_data[dataset_data['significant'] == True]
                if len(sig_results) > 0:
                    f.write(f"  Significant findings:\n")
                    for _, row in sig_results.iterrows():
                        direction = "â†‘" if row['tumor_median'] > row['normal_median'] else "â†“"
                        f.write(f"    {row['ptm']} - {row['property']}: p_adj={row['p_adj']:.4f} {direction}\n")

            # è·¨æ•°æ®é›†ä¸€è‡´æ€§åˆ†æ
            f.write(f"\nCROSS-DATASET CONSISTENCY\n")
            f.write("-" * 20 + "\n")

            # è®¡ç®—æ¯ä¸ªPTM-propertyç»„åˆåœ¨å¤šå°‘ä¸ªæ•°æ®é›†ä¸­æ˜¾è‘—
            consistency_results = []

            for ptm in summary_df['ptm'].unique():
                for prop in summary_df['property'].unique():
                    subset = summary_df[(summary_df['ptm'] == ptm) & (summary_df['property'] == prop)]
                    if len(subset) > 0:
                        sig_datasets = subset[subset['significant'] == True]['dataset_id'].tolist()
                        total_datasets = len(subset)
                        consistency_rate = len(sig_datasets) / total_datasets

                        consistency_results.append({
                            'ptm': ptm,
                            'property': prop,
                            'significant_datasets': len(sig_datasets),
                            'total_datasets': total_datasets,
                            'consistency_rate': consistency_rate,
                            'datasets': sig_datasets
                        })

            # æŒ‰ä¸€è‡´æ€§æ’åº
            consistency_results.sort(key=lambda x: x['consistency_rate'], reverse=True)

            f.write("Most consistent findings across datasets:\n")
            for result in consistency_results[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                if result['consistency_rate'] > 0:
                    f.write(f"  {result['ptm']} - {result['property']}: ")
                    f.write(f"{result['significant_datasets']}/{result['total_datasets']} datasets ")
                    f.write(f"({result['consistency_rate']*100:.1f}%)\n")
                    if result['datasets']:
                        f.write(f"    Datasets: {', '.join(result['datasets'])}\n")

            # æ•ˆåº”é‡åˆ†æ
            f.write(f"\nEFFECT SIZE ANALYSIS\n")
            f.write("-" * 20 + "\n")

            # è®¡ç®—å¹³å‡æ•ˆåº”é‡
            for ptm in summary_df['ptm'].unique():
                ptm_data = summary_df[summary_df['ptm'] == ptm]

                if 'cliff_delta' in ptm_data.columns:
                    mean_effect = ptm_data['cliff_delta'].mean()
                    f.write(f"{ptm} average effect size (Cliff's delta): {mean_effect:.3f}\n")

                    # æŒ‰propertyåˆ†æ
                    for prop in ptm_data['property'].unique():
                        prop_data = ptm_data[ptm_data['property'] == prop]
                        prop_effect = prop_data['cliff_delta'].mean()
                        f.write(f"  {prop}: {prop_effect:.3f}\n")

            f.write(f"\nFILES GENERATED\n")
            f.write("-" * 20 + "\n")
            f.write(f"- dataset_level_summary.csv: Raw data for all comparisons\n")
            f.write(f"- dataset_level_significance_heatmap.png: Significance patterns\n")
            f.write(f"- dataset_level_effect_sizes.png: Effect size distributions\n")
            f.write(f"- dataset_sample_distribution.png: Sample size information\n")
            f.write(f"- dataset_level_analysis_report.txt: This report\n")

        print(f"      ä¿å­˜æŠ¥å‘Š: dataset_level_analysis_report.txt")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ è‚¿ç˜¤ vs æ­£å¸¸æ ·æœ¬ PTM æ•ˆåº”æ¯”è¾ƒåˆ†æ")
    print("=" * 60)

    # åˆ›å»ºåˆ†æå™¨
    analyzer = TumorNormalPTMAnalyzer()

    # å¯ä»¥æŒ‡å®šç‰¹å®šçš„PTMè¿›è¡Œåˆ†æ
    target_ptms = ['Phospho[S]', 'Acetyl[K]', 'Deamidated[N]']

    try:
        # åˆ†ææ‰€æœ‰åŒ…å«Tumorå’ŒNormalæ ·æœ¬çš„æ•°æ®é›†
        analyzer.analyze_all_datasets(target_ptms)

        print("\nğŸ¯ åˆ†æå®Œæˆï¼")
        print("ğŸ“Š æŸ¥çœ‹ç»“æœ:")
        print(f"  - å„æ•°æ®é›†ç»“æœ: tumor_vs_normal_ptm_results/[DATASET_ID]/")
        print(f"  - æ€»ä½“æŠ¥å‘Š: tumor_vs_normal_ptm_results/overall_tumor_vs_normal_report.txt")

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
